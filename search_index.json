[["index.html", "The portfolio of Nicky Klaver Introduction", " The portfolio of Nicky Klaver Introduction This is the portfolio of Nicky Klaver, my github book you can find on these words :D. In this portfolio you can see the different assignments that have been presented to me, and you can see my results in this document. This includes document includes: My Curicullum Vitaé My free space assignment where I learn a self taught skill My Guerilla analytics workflow My troubleshooting capabilities on C. elegans analysis My self made R package with 4 functions with documentation My parameters skills performed on ECDC data Enjoy! "],["cv.html", "1 CV", " 1 CV Why me? My motivation to study Biomedical sciences stems from the deep unknown of the human body and all its functions. During my studies I was fascinated by the body and its capabilities. My future goal/ambition is to contribute to scientific discoveries. Through my experience with working in diverse catering/food industries/companies, especially in the meat sector, I have obtained a quality-oriented mindset, am able to work securely and safely with a keen eye for detail and with customer and company satisfaction as goal. Some stuff about me: I am fluent in Dutch and English. I am an intermediate in software skills program R and Bash. I do Thai boxing 1-2 times a week, with addition weightlifting when possible. I love learning new things and when possible implementing those newly learned things. Education HBO Bachelor biologie en medisch laboratoriumonderzoek - Hogeschool Utrecht Utrecht: 2021-Present Completed propedeuse in first year, completed specialism BMR, with great affection to tumor cell biology, and proceeding a minor in Data Science, working with R and Bash. The diverse technologies, skills, and knowledge that I have developed in my bachelor studies are: GMO &amp; cell culture Recombinant DNA &amp; protein skills e.g. SDS-PAGE &amp; western blot HPLC &amp; TLC Tumor cell biology Bash, HTML and R programming, also working in a workflow with scrum and agile My ambition after Life Sciences is to discover and research the unknown of the human body, and most particularly in tumor cell biology and oncology with a future focus of creating cures for different cancers. Havo - Laar en Berg: Laren: 2015-2021 Completed path Natuur &amp; Techniek and Natuur &amp; gezonheid, with first 3 years of MYP (dual language program) with English as second language. Ambition/ future focus Previous jobs Slagerij Klaver &amp; Zn. - Bussum: May 2023–Present Employee and media manager, responsible for sales of products and the online media accounts and activities. At this job I learned how to communicate with people, and how to express this online, and in real life. The eye for detail and cleanliness but also the customer satisfaction are really helpfull in this bussiness, seeing as I am working with different foods, but also working with a lot of customers through the day. Studentaanhuis - Utrecht May 2023–Present Call center employee, responsible for delivering clear communication to customers and setting appointments with IT specialist and customers. At this job I learned how to communicate with detailed precision and also how to listern carefully and precisely to match the correct IT specialist to the customer. Customer satisfaction is a skill is developed further. Picnic - Bussum August 2021–November 2021 Grocery courier, part-time during start of my bachelor. I learned here how to be quick and efficient, and customer satisfaction was a skill is developed here. Nice to Meat - Almere June 2017–May 2023 Production assistant leader, responsible for large supply of meat to catering companies. During my time in Middle School, on Saturdays and holidays. Here I really learned how to work under pressure. Especially on Saturdays where is was not the leader of the day and had a team of 3+ people working under me. Cleanliness and hygiene was nothing but important here, and working with food grade materials made the cleaning and the end of the day just much more precise. Here I developed an eye for detail and cleanliness. Spareribs Express - Hilversum September 2016–June 2017 Kitchen help/ assistant. During my time in Middle School, weekdays, and weekends. Here I learned how to work in an organisation that was working around the clock, not only having to be consistent in the workflow, but also being on time, organising the right preperations and also working under pressure. "],["free-space-assignment.html", "2 Free space assignment Introduction single cell RNA sequencing", " 2 Free space assignment Introduction single cell RNA sequencing At the start of my workflows course, I was presented by my teachers to perform a free space assignment. And as it sounds, it was completely free. It had to be Life Science related, and you had to learn something from it! The questions that are noted below where asked to me, and the answers I gave are shown under it. Where do I want to be in ±2 years? I want to be able to execute a single cell sequencing dataset analysis and be able to use my git and github repocitories to make it reproducable for other people. How am I doing now with respect to this goal? I am learning how to use git and github, and I am thus making my work more reproducible. What would be the next skill to learn? I would need to learn how to analyse single cell sequencing, and also how to manipulate and communicate those results and visualize them as well. Planning: I have to find a single cell Seq. dataset and I have to inspect if it is something that I can work with. I have to make the data tidy, then analyse the data with R and with bash. I have to gather the results, and interpret these results into a graph of chart. I have to put all this in a Rmarkdown which can be checked, and reproduced, only having to insert the used data set. I have to work via the workflow protocol, meaning with Git and Github. May 8th 2-4 hours: Finding the read count tables, and looking for associated code. Inspecting the data if it is usable. May 14th 6-8 hours: How do I analyse this read count, and can I make an expression profile of these read counts. May 15th 3-4 hours: Continue creating a expression profile, or if already finished, start identifying the type of cell. May 21st 6-7 hours: Continue identifying the type of cell that is expressed by its expression profile. May 26th 2 hours: Finish off the free space assignment, make sure everything is nice and the layout sits nice of the portfolio. Start of the free time project: I started looking for YouTube videos that could give me some insights on how to perform Single Cell RNA Sequencing for some background information. It started with the following video’s (same creator, its a series, but not everything was as need so I filtered a little bit: (Ep. 1) (SIB - Swiss Institute of Bioinformatics 2023f) (Ep. 3) (SIB - Swiss Institute of Bioinformatics 2023a) (Ep. 4 halverwege gestart, daar was uitleg over wat je in R kon doen met bijvoorbeeld een PCA analysis, en met de package Seurat wat de functies daar van waren.) (SIB - Swiss Institute of Bioinformatics 2023e) (Ep. 6 clustering) (SIB - Swiss Institute of Bioinformatics 2023c) (Ep. 7 Cell type anotation) (SIB - Swiss Institute of Bioinformatics 2023b) (Ep. 8 DGE analysis) (SIB - Swiss Institute of Bioinformatics 2023d) I gained information about what the workflow consists of, some different packages and also what some different analysis were that I could perform on the data set once I found a count table / count matrix. Then I proceeded to watch the following video. From this video I gained information about meaning of some words, and again packages that I could use for analysis of Single Cell RNA Sequences Meaning of words UMI = Unique Molecular Identifiers, molecular tags that can be applied to detect and quantify the unique transcripts. Features = genes Bar codes = Single-cell sequencing experiments use short DNA bar code ‘tags’ to identify reads that originate from the same cell. doublets = when two cells are encapsulated into one reaction volume. R packages for Single cell RNA sequencing SingleCellExperiment Seurat monocle3 Scater After the YouTube video I went to search on her github page for some packages or data sets that I could use, which could or could not have included code and a count table. Some time had past and I found a promising repository, and upon reading the README file is discovered some insights about also coursed and other things that I could follow to perform a Single Cell RNA Sequence: (“Awesome-Single-Cell/README.md at Master \\(\\cdot\\) Seandavi/Awesome-Single-Cell” n.d.) The cited course emerged in my hands, and I was eager to see what it could do for me (“Cellgeni/scRNA.seq.course” 2024). I was snooping around in all the files, and when looking at the README file I saw that there was a Webpage, and also a shared Data folder set! This of course is perfect for me, because I was needed to find a data set, with a read count table so that I could perform the analysis. When looking at the website I saw the (presumably) read count table, it was named GSE149938_umi_matrix.csv.gz. This file was put into the raw data folder, and a copy was placed into the data folder. I was searching for any sign of the Umi matrix being in correspondence with the read count table, and I came across the following website that said: “Sequencing data from single-cell RNA-seq experiments must be converted into a matrix of expression values. This is usually a count matrix containing the number of reads mapped to each gene (row) in each cell (column). Alternatively, the counts may be that of the number of unique molecular identifiers (UMIs); these are interpreted in the same manner as read counts but are less affected by PCR artifacts during library preparation” Chapter 3 Getting scRNA-seq Datasets Introduction to Single-Cell Analysis with Bioconductor (n.d.) Revealing the Umi Matrix The UMI (Unique Molecular Identifier) matrix I downloaded was input into the Umi_matrix_NK object for further analysis. Downloaded from “S3 Browser Singlecellcoursedata/” (n.d.). Umi_matrix_NK &lt;- read.csv(here::here( &quot;data&quot;, &quot;GSE149938_umi_matrix.csv.gz&quot;)) I found that there were multiple ways to import matrix data into R, one example I came up with myself was the above version. After further notice, and research, I found out this might not have been the best way. I had read on the bioconducter website that I could best use read.delim or use the package scuttle and then use the function readSparseCounts because of the non zero-values not being taken into consideration. When looking at the Umi_matrix_NK object, I saw a lot of zero values, so I thought this could come in handy. I performed the following commands: if (!require(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) if (!require(&quot;scuttle&quot;, quietly = TRUE)) BiocManager::install(&quot;scuttle&quot;) library(scuttle) sparse_matrix_NK &lt;- readSparseCounts(here::here(&quot;data&quot;,&quot;GSE149938_umi_matrix.csv.gz&quot;)) dim(sparse_matrix_NK) # check the dimensions But when trying out the code I got the following error message: Error in as(do.call(cbind, current[cell.cols]), “dgCMatrix”) : no method or default for coercing “NULL” to “dgCMatrix” And when you thought things couldn’t get any worse, git and github decided to not work as well. This because of the fact that on the Umi_matrix website, there also was another file called matrix, which I downloaded onto my computer. I had put this into my raw_data folder, and copied it into my data folder. When I tried to push my changes to github, it gave me an error. I kinda ignored this error for another 2 commits before I checked what it said. The file was too big, it didn’t upload. This of course was a problem, but I found out I didn’t need this (300 MB) matrix file (I also had it twice so make that 600 MB). I tried a lot of different things, but it just did not work. I tried in the terminal, I tried in the git section in the top right. Nothing was working for me. In the terminal I looked for the git help page, and that did not do anything for me like a quick fix. It looked like I would be stuck for a little while. Some awful time later on google and github forums I found the following solution (Hallelujah): The answer is simple: use the git reset –soft HEAD~1 command. This command will undo your last commit, but it will keep your changes and your staging intact. So I tried, and nothing worked, so I typed it in again, and again, and again, and then I even changed it to HEAD~3 to make it every 3 commits. Then all of a sudden it all came back, my last (I think) 10 commits onto my screen. I repushed, and it stopped me in my tracks. I was thinking, when will this end. Then I did: git pull git add --all git push And now finally, after all this work. I was back to only my original problem. The readSparseCounts problem. I went back to the readSparseCounts and saw that it did not work, so I went to look for how to load in UMI counts. I looked at the same website from the readSparseCount command and saw another method: UMI_Matrix_NK &lt;- as.matrix(read.csv(here::here(&quot;data&quot;,&quot;GSE149938_umi_matrix.csv.gz&quot;))) dim(UMI_Matrix_NK) # shows the dimension of the matrix. Now after finally being able to load in the matrix, it was time to work on analyzing the matrix. Analyse the Unique Molecular Identifier (UMI) matrix For the analysis for single cell RNA sequencing we shall use the SingleCellExperiment package that was noticed in 1 of the first video’s watched. Also we installed the Scater package for further analysis. And last but not least also the Seurat package was installed, because this was showcased in the video’s of the Swiss Institute of Bioinformatics. if (!require(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) if (!require(&quot;Seurat&quot;, quietly = TRUE)) install.packages(&#39;Seurat&#39;) library(Seurat) if (!require(&quot;SingleCellExperiment&quot;, quietly = TRUE)) BiocManager::install(&quot;SingleCellExperiment&quot;) library(SingleCellExperiment) if (!require(&quot;scater&quot;, quietly = TRUE)) BiocManager::install(&quot;scater&quot;) library(scater) Now that we have installed and downloaded and loaded in the different packages, we can start by reading the documentation to see what functions it has. This was done by reading the manual online, as well as performing ?\"insert package\" function in the terminal. Analysis overview First things first, we have to perform an analysis overview, which includes: Cell filtering (quality control), Normalization, Feature selection, Scaling, Dimensionality reduction, Clustering, annotation, DGE, etc. Cell filtering The following function was performed: # SingleCellExperiment function start of to make it an matrix with metadata of the rows and columns. SCE_UMI_NK &lt;- SingleCellExperiment(list(counts=UMI_Matrix_NK)) SCE_UMI_NK I came across another files set: SCE_Reads_NK &lt;- readRDS(here::here(&quot;raw_data&quot;,&quot;reads.rds&quot;)) scRNA course!?! At this point in time I found that I was allowed to follow a course on scRNA, and that I should be able to perform it on a different data set. The course was presented to my on the following website. On this website I would be able to follow the course, and perform the analysis on a different data set as I went along, or so I thought. There was another website where from I could get my data, that was right here (or so I thought). It seemed to me that all the data sets where different, they all just did not want to work with me. I did not give up hope, but it was kind of dissapointing because I had the feeling I could have gotten a lot further would’ve this dataset just worked with me like I wanted it to (# probably every scientist ever has said this). Seurat analysis After all the research, fideling, problems and etc. It was time for me to finally analyse. It had waited long enough. I found on the Seurat command list website the “normal” Seurat workflow, and I went with it. The Seurat website was found here. I started by loading in the UMI matrix that I first found. I loaded it into a normal object with the here function and read.csv. UMI_object &lt;- read.csv(here::here( &quot;data&quot;, &quot;GSE149938_umi_matrix.csv.gz&quot;)) After the data import I created a Seurtobject from the UMI matrix. Seurat_object_UMI &lt;- CreateSeuratObject(counts = UMI_object) After creating the Seurat object it said that it could not have underscores for the feautures but dashes, so it replaced these. Also the data is of class data frame, so it said “Coercing to dgCMatrix. This changing to a sparse matrix is I figured a problem, it would probably be more efficient. Seurat pre-processing After the Seurat object was created I performed some pre-processing steps to the data like normalization, identify highly variable genes, and scaling the data. The function NormalizeData is one function of the Seurat packages, and it performs a log-normalization on the count data given in the created seuratobject. Seurat_object_UMI &lt;- NormalizeData(Seurat_object_UMI) After this we want to identify high variable ‘features’ (genes), and we can use the function FindVariableFeatures. This function will identify features that are outliers on a ‘mean variability plot’.@FindVariableFeatures Seurat_object_UMI &lt;- FindVariableFeatures(Seurat_object_UMI) Another important preprocessing step is the scaling of the data. For this we use another Seurat function called ScaleData. This function scales and centers features in the dataset. “Scale and Center the Data. — ScaleData” (n.d.) Seurat_object_UMI &lt;- ScaleData(Seurat_object_UMI) Seurat dimensionality reduction and clustering As seen above here, Seurat is a package with a lot of functions to perform pre-processing on UMI matrixes. But it does not stop there. This is because there are also dimensionality reduction and clustering functions ready to be used. The first one is the RunPCA function, this function will change the dimensions of the object and reduce them to a minimal. Seurat_object_UMI &lt;- RunPCA(Seurat_object_UMI) The RunPCA function has as a default that it prints the first 5 Principle Components and shows the positives and the negatives (the features inside those dimensions, and the features outside those dimensions). For the following analysis I will use 2 different objects and see which one is better. For this analysis we will look at the RunUMAP function. This runs the Uniform Manifold Approximation and Projection (UMAP) dimensional reduction technique.@RunUMAPRunUMAP The UMAP technique: “(UMAP) is a dimensionality reduction technique that constructs a high dimensional graph representation of the data then creates a low-dimensional graph to be as structurally similar as possible.@katherine-rittenbachWhatUMAP Because it could be wise to select the 10 most important principle components we could use this function with the argument dims = 1:10. But because I don’t want to lose the perhaps valuable information that is stored in the other dimensions I will also run the function with the argument dims = 1:30 to be able to compare the data against each other and see if the dimensions make such a big difference. I will do this by performing the functions RunUMAP twice, and then follow up with the functions FindNeighbours and FindClusters, which are also functions of the Seurat Package. After this I will visualize both with the DimPlot function of (again) the Seurat package. # Perform the Seurat object UMAP on 10 dimensions Seurat_object_UMI_10 &lt;- FindNeighbors(Seurat_object_UMI, dims = 1:10) Seurat_object_UMI_10 &lt;- FindClusters(Seurat_object_UMI_10) ## Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck ## ## Number of nodes: 19813 ## Number of edges: 2776300 ## ## Running Louvain algorithm... ## Maximum modularity in 10 random starts: 0.2565 ## Number of communities: 2 ## Elapsed time: 9 seconds Seurat_object_UMI_10 &lt;- RunUMAP(Seurat_object_UMI, dims = 1:10) # Perform the Seurat object UMAP on 30 dimensions Seurat_object_UMI_30 &lt;- FindNeighbors(Seurat_object_UMI, dims = 1:30) Seurat_object_UMI_30 &lt;- FindClusters(Seurat_object_UMI_30) ## Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck ## ## Number of nodes: 19813 ## Number of edges: 3036394 ## ## Running Louvain algorithm... ## Maximum modularity in 10 random starts: 0.2584 ## Number of communities: 3 ## Elapsed time: 11 seconds Seurat_object_UMI_30 &lt;- RunUMAP(Seurat_object_UMI, dims = 1:30) # Visualize the dimensionality reduction and compare with gridarrange. Seurat_10 &lt;- DimPlot(Seurat_object_UMI_10, reduction = &quot;umap&quot;) Seurat_30 &lt;- DimPlot(Seurat_object_UMI_10, reduction = &quot;umap&quot;) grid.arrange(Seurat_10, Seurat_30, ncol = 2) Looking at the 2 graphs, it looks like there is not real difference to using only the 10 dimensions, so it means I can further with that. Seurat expression profile Now that pre-processing is finished, lets make it so that we can identify different cluster markers. The Seurat function FindAllMarkers finds markers (differentially expressed genes) for each of the identity classes in a dataset.@GeneExpressionMarkers We want to identify each marker genes of each cluster. # First run again the function FindNeighbours and FindClusters Seurat_object_UMI_10_c &lt;- FindNeighbors(Seurat_object_UMI, dims = 1:10) Seurat_object_UMI_10_c &lt;- FindClusters(Seurat_object_UMI_10_c) ## Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck ## ## Number of nodes: 19813 ## Number of edges: 2776300 ## ## Running Louvain algorithm... ## Maximum modularity in 10 random starts: 0.2565 ## Number of communities: 2 ## Elapsed time: 9 seconds # Find all the markers UMI_Markers &lt;- FindAllMarkers(Seurat_object_UMI_10_c, only.pos = T, # Only return positive markers. min.pct = 0.25, # Higher than 0.01 to speed up the process and find only genes that are not rarely expressed. logfc.threshold = 0.25) # Higher then 0.1 to speed up process, small change of missing weaker signals. After calculating the markers, we want to take out the top 10 markers for each cluster with the top_n function of the dplyr package. # Select top set of rows with top_n() top_10markers_UMI &lt;- UMI_Markers %&gt;% group_by(cluster) %&gt;% dplyr::top_n(n = 10, # 10 rows picked wt = avg_log2FC) # The variable used for ordering. # Print the top 10 genes print(top_10markers_UMI) ## # A tibble: 20 × 7 ## # Groups: cluster [2] ## p_val avg_log2FC pct.1 pct.2 p_val_adj cluster gene ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; ## 1 0 14.3 0.442 0 0 0 ery-eBM7-L30-bar50 ## 2 1.36e-254 14.3 0.382 0 1.04e-250 0 LMPP-spBM2-L8-bar51 ## 3 7.07e-250 15.4 0.377 0 5.40e-246 0 ery-eBM1-L9-bar36 ## 4 4.11e-249 13.9 0.376 0 3.14e-245 0 ery-eBM5-L27-bar93 ## 5 5.00e-247 14.0 0.373 0 3.82e-243 0 ery-eBM6-L22-bar41 ## 6 2.70e-244 13.9 0.37 0 2.07e-240 0 GMP-spBM2-L7-bar29 ## 7 2.79e-225 13.9 0.349 0 2.13e-221 0 HSC-spBM1-L6-bar68 ## 8 1.58e-218 14.0 0.341 0 1.21e-214 0 CMP-spBM1-L8-bar38 ## 9 3.65e-203 14.0 0.322 0 2.79e-199 0 ery-eBM5-L28-bar68 ## 10 2.77e-202 14.1 0.321 0 2.11e-198 0 GMP-spBM1-L7-bar74 ## 11 6.23e-212 3.59 0.013 0.36 4.76e-208 1 LMPP-spBM1-L1-bar49 ## 12 1.00e-187 3.12 0.01 0.324 7.67e-184 1 BNK-spBM1-L2-bar49 ## 13 7.53e-165 2.83 0.004 0.282 5.76e-161 1 BNK-spBM1-L1-bar48 ## 14 1.20e-164 2.72 0 0.274 9.13e-161 1 cMOP-moBM3-L6-bar81 ## 15 1.50e-162 3.34 0.005 0.28 1.15e-158 1 LMPP-spBM1-L1-bar63 ## 16 1.64e-157 3.56 0.002 0.269 1.25e-153 1 MPP-spBM1-L2-bar21 ## 17 3.67e-155 2.90 0.001 0.263 2.80e-151 1 proB-bBM2-L15-bar35 ## 18 1.25e-147 2.85 0.003 0.256 9.58e-144 1 HSC-spBM1-L1-bar20 ## 19 7.57e-144 3.13 0.003 0.251 5.78e-140 1 LMPP-spBM1-L4-bar11 ## 20 6.21e-140 2.80 0.006 0.251 4.75e-136 1 MPP-spBM1-L4-bar61 After now seeing the top 10 markers for both clusters, we can now visualize and compare these results. There are multiple ways to do this, but I will use the heatmap as learned in the DAUR2 lessons. Luckily, the Seurat package already has a function for this called DoHeatmap. UMI_Heatmap &lt;- DoHeatmap(Seurat_object_UMI_10_c, features = top_10markers_UMI$gene) UMI_Heatmap Sadly, after looking at the UMI_Heatmap, most of the features of cluster 1 were not taken into consideration because they were not found in the scale.data slot for the RNA assay. Luckily (again), the Seurat package had another way of visualizing the features. This was either with the DotPlot function. The DotPlot is an intuitive way of visualizing how feature expression changes across different identity classes (clusters). The size of the dot encodes the percentage of cells within a class, while the color encodes the AverageExpression level across all cells within a class (blue is high). “Dot Plot Visualization — DotPlot” (n.d.) # Perform the Dotplot on the UMI object UMI_Dot &lt;- DotPlot(Seurat_object_UMI_10_c, features = top_10markers_UMI) Sadly, also this command did not work, seeing as it gave the error: Error in levels&lt;-(*tmp*, value = as.character(levels)) : factor level [54] is duplicated The visualization did not really work for me. I tried to find the so called duplicates, but in the top10markers_UMI there were NO duplicates. Even tho I did not get the visualization that I wanted. I did find a website here where I could look up the feauture that I had found, so I tried that. And that was for now the end of my free space assignment. I am eager to continue pursuing this route and hopefully finding a way to present that visualizations. Not IF but WHEN I get those visualizations, I will send them to my mentors. References “Awesome-Single-Cell/README.md at Master \\(\\cdot\\) Seandavi/Awesome-Single-Cell.” n.d. GitHub. https://github.com/seandavi/awesome-single-cell/blob/master/README.md. Accessed May 14, 2024. “Cellgeni/scRNA.seq.course.” 2024. Cellular Genetics Informatics. Chapter 3 Getting scRNA-seq Datasets Introduction to Single-Cell Analysis with Bioconductor. n.d. Accessed May 14, 2024. “Dot Plot Visualization — DotPlot.” n.d. https://satijalab.org/seurat/reference/dotplot. Accessed May 26, 2024. “S3 Browser Singlecellcoursedata/.” n.d. https://singlecellcourse.cog.sanger.ac.uk/index.html?shared=data/. Accessed May 15, 2024. “Scale and Center the Data. — ScaleData.” n.d. https://satijalab.org/seurat/reference/scaledata. Accessed May 26, 2024. SIB - Swiss Institute of Bioinformatics. 2023a. “Single Cell Transcriptomics - Analysis Tools and Quality Control (3 of 10).” ———. 2023b. “Single Cell Transcriptomics - Cell Type Annotation (7 of 10).” ———. 2023c. “Single Cell Transcriptomics - Clustering (6 of 10).” ———. 2023d. “Single Cell Transcriptomics - Differential Gene Expression and Enrichment Analysis (8 of 10).” ———. 2023e. “Single Cell Transcriptomics - Dimensionality Reduction (4 of 10).” ———. 2023f. “Single Cell Transcriptomics - Introduction to Single Cell RNA-seq (1 of 10).” "],["guerilla-analytics.html", "3 Guerilla analytics Print root of DAUR2 project Root of DAUR2 project Print root of workflows portfolio project", " 3 Guerilla analytics When working on a data science project, it is important to have a workflow of some sort, that keeps your data tidy, and where it should be. For me this is working with the guerrilla analytics. Guerrilla analytics is working with a set of folder structures and 7 principles that make it so that every project is reduced to as less of a chaos as possible. The 7 principles are as followed: Space is cheap, confusion is expensive Use simple, visual project structures and conventions Automate with program code Link stored data to data in the analytics environment to data in work products Version control changes to data and analytics code Consolidate team knowledge Use code that runs from start to finish When complying to these principles, it as almost assured that there is little to no chaos in your workflow, and that working within this project is the easiest you have ever done. Everything should be in precisely the right place, where you always leave it. Print root of DAUR2 project Below here is how you can see the root of my DAUR2 project from my own computer. # Print the root of the DAUR2 project from own computer. fs::dir_tree(here::here(&quot;/Users/nickyklaver/Desktop/00_Life_Science/Jaar_3/Data_Science/rstudio_Daur2&quot;)) That is ofcourse not so smart, because you (the person reading this) is not on my computer. Please see below the pictures made from the root. Root of DAUR2 project Figure 3.1: Part 1 of the root of the DAUR2 project of Nicky Klaver Figure 3.2: Part 2 of the root of the DAUR2 project of Nicky Klaver Figure 3.3: Part 3 of the root of the DAUR2 project of Nicky Klaver Print root of workflows portfolio project Also for this current portfolio there is a root. You can see that this portfolio also has the same guerilla analytics principles applied. fs::dir_tree(&quot;.&quot;, recurse = TRUE, regexp = &quot;^.gitignore$|^[^_|(.git)]&quot;, all = TRUE) ## . ## ├── .gitignore ## ├── 01_CV_Nicky_Klaver.Rmd ## ├── 02_Free_Space_Portfolio_NK.Rmd ## ├── 03_Guerilla_Analytics.Rmd ## ├── 04_C_Elegans_Plate.Rmd ## ├── 05_Open_Peer_Review.Rmd ## ├── 06_easyRead_package.Rmd ## ├── 07_Project_introduction.Rmd ## ├── 08_Parameters_ECDC.Rmd ## ├── 20_bibliografie.Rmd ## ├── LICENSE ## ├── Packages_Vrije_opdracht.R ## ├── README.md ## ├── Vrije_opdracht.bib ## ├── data ## │ ├── CE.LIQ.FLOW.062_Tidydata.xlsx ## │ ├── GSE149938_umi_matrix.csv.gz ## │ ├── Metadata_data.txt ## │ └── README_data.txt ## ├── dsfb2_workflows_portfolio.Rproj ## └── raw_data ## ├── .DS_Store ## ├── CE.LIQ.FLOW.062_Tidydata.xlsx ## ├── GSE149938_umi_matrix.csv.gz ## ├── README_rawdata.txt ## ├── Random_data_R_package.csv ## ├── data.csv ## └── reads.rds "],["portfolio-assignment-1.1-c-elegans-plate.html", "4 Portfolio assignment 1.1 C Elegans Plate Anything peculiar? Read in excel file in R Inspect the data types of the following columns Create a graph displaying a scatterplot Make character into numeric Further analysis normalize the data to the negative control Why normalize?", " 4 Portfolio assignment 1.1 C Elegans Plate Anything peculiar? When starting with an analysis, inspecting the data is essential. If you do not first inspect the data, then you do not know what you are looking for, of what you are already missing. The point of inspecting data is to comprehend, and to look for anything out of the ordinary. When inspecting the data from the C. Elegans plate (which was given to us in an excel file), we can see the following: In the RawData column, there are some values that are 0, and values that are blank. The first 2 columns ‘plateRow’ &amp; ‘plateColumn’ are empty of values. The data set is in a tidy format: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. The column ‘incubationVolume’ has a numeric value and a character value, which makes it no really machine readable. Read in excel file in R After inspection it is time to read in the data frame into an object with the read_excel function in the readxl package. After reading in the package, I always like to check in a grammar of tables or in a reactable that all my data is sorted nicely. You could also use view() but it prefer it this way. # Import the data with the read_excel function from the raw_data folder CE_LIQ_FLOW_062_TidyData &lt;- readxl::read_excel( here::here( &quot;raw_data&quot;, &quot;CE.LIQ.FLOW.062_Tidydata.xlsx&quot;)) # Make a reactable to see what the data looks like in R reactable(data = CE_LIQ_FLOW_062_TidyData, defaultPageSize = 5, compact = T, highlight = T) Inspect the data types of the following columns We also would like to aspect the class of the used data. If a data type should be numeric, but because it was imported via an excel file and it transposed it into a character, it would be much harder for computer to do something with this data, because it is not presented as it should. In this example, the comp concentration data should be numeric, but is presented as character. This is a classic example of data importing in R and giving the wrong data type to the columns. class(CE_LIQ_FLOW_062_TidyData[[&quot;RawData&quot;]]) ## [1] &quot;numeric&quot; class(CE_LIQ_FLOW_062_TidyData[[&quot;compName&quot;]]) ## [1] &quot;character&quot; class(CE_LIQ_FLOW_062_TidyData[[&quot;compConcentration&quot;]]) ## [1] &quot;character&quot; Create a graph displaying a scatterplot CE_LIQ_FLOW_062_TidyData_U &lt;- CE_LIQ_FLOW_062_TidyData %&gt;% ggplot(aes(x = compConcentration, y = RawData)) + geom_point(aes( shape = expType, colour = compName)) + toolboxr::rotate_axis_labels(&quot;x&quot;, 45) # Adjusts the angle of the X-axis so it becomes human readable CE_LIQ_FLOW_062_TidyData_U When creating the scatter plot shown above, we see that the X-axis has values that do not comprehend with each other. They are not linear, or exponential, or anything related to each other. This is caused by the datatype character of column compound concentration. Make character into numeric To change this character into numeric we need to use the mutate function to change the column comp concentration, but to further change the graph we turn the x-axis into a logarithmic axis. CE_LIQ_FLOW_062_TidyData_C &lt;- CE_LIQ_FLOW_062_TidyData %&gt;% mutate(compConcentration = as.numeric(compConcentration)) %&gt;% ggplot(aes(x = log10(compConcentration + 0.001), y = RawData)) + geom_point(aes(colour = compName, shape = expType), position = position_jitter(w = 0.05, h = 0)) + toolboxr::rotate_axis_labels(&quot;x&quot;, 45) # Adjusts the angle of the X-axis so it becomes human readable CE_LIQ_FLOW_062_TidyData_C ## Positive and negative controls{-} Seen in the graph is that a triangle is the positive control, for this experiment it is Ethanol. Seen in the graph is that the circle is the negative control, for this experiment it is S-medium. Further analysis To really further analyse this data, you would have to put each compound out against each RawData sample, so you can see the IC50 and its curve. We will start by 1. selecting and dividing the compounds and adding each compound concentration, with each RawData (offspring) that is set to it, into a new data frame or tibble. After that we 2. will make a scatter plot, and the dots on the plot will represent the amount of offspring at that specific concentration. Then we 3. create a line at the 50% mark of the inhibition (50% less offspring) by calculating the offspring with and without the added substance, and taking half of this. If we do this for all the compounds we can look at the different inhibitory functions of the compounds and from here conclude what the effect the different effects of the compounds is on the offspring and the concentration of this compound. When performing other research, you know that this IC50 can be useful in testing so you know for an (almost) certain amount that when using that concentration that 50% less offspring will be there. normalize the data to the negative control # filter on &quot;controlNegative&quot; data CE_LIQ_FLOW_062_CNeg &lt;- CE_LIQ_FLOW_062_TidyData %&gt;% filter(expType == &quot;controlNegative&quot;) # calculate mean of &quot;RawData&quot; of the filtered dataset CNeg_mean &lt;- mean(CE_LIQ_FLOW_062_CNeg$RawData) # check contNeg_mean CNeg_mean ## [1] 85.9 # create a normalised column with the normalised values. CE_LIQ_FLOW_062_Norm &lt;- CE_LIQ_FLOW_062_TidyData %&gt;% mutate(normalised = RawData/CNeg_mean) # mean RawData mean(CE_LIQ_FLOW_062_Norm$RawData, na.rm=T) ## [1] 68.10423 # check normalised column CE_LIQ_FLOW_062_Norm %&gt;% select(compName, expType, compConcentration, RawData, normalised) %&gt;% head() ## # A tibble: 6 × 5 ## compName expType compConcentration RawData normalised ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2,6-diisopropylnaphthalene experiment 4.99 44 0.512 ## 2 2,6-diisopropylnaphthalene experiment 4.99 37 0.431 ## 3 2,6-diisopropylnaphthalene experiment 4.99 45 0.524 ## 4 2,6-diisopropylnaphthalene experiment 4.99 47 0.547 ## 5 2,6-diisopropylnaphthalene experiment 4.99 41 0.477 ## 6 2,6-diisopropylnaphthalene experiment 4.99 35 0.407 CE_LIQ_FLOW_062_CNeg_Norm &lt;- CE_LIQ_FLOW_062_Norm %&gt;% filter(expType == &quot;controlNegative&quot;) CNegNorm_mean &lt;- mean(CE_LIQ_FLOW_062_CNeg_Norm$normalised) # check if mean value is 1 CNegNorm_mean ## [1] 1 Why normalize? We have just normalized the data of the control negative for where the mean of that is 1. The purpose of this is to improve the dataset, and make it more accurate. It will also center our data, so that we can visualize it better when e.g. making a plot. "],["open-peer-review.html", "5 Open Peer Review Initial peer review criteria listing The second peer review with code and data", " 5 Open Peer Review A researchers quality is to be able to peer review the work of others. For this quality I am going to perform 2 peer reviews. One of the reviews is going to be me giving a score to a paper, and for this using the criteria that is publicly available here. These criteria will determine how good of a reproducible paper, and what a open science paper, should be. After this initial review, I will look at another paper, and I will look for code and for data to see how reproducible this code and data is to give me the same outcome as the researcher. How many errors do I have to fix to make it work? How easy was it to fix these errors? I will also use the criteria listed, but I will also perform the analysis below here. Initial peer review criteria listing To find the first paper you can click here. The authors of this paper are Petek Piner Benli &amp; Mehmet Celik. The title is: “In Vivo Effects of Neonicotinoid-Sulfoximine Insecticide Sulfoxaflor on Acetylcholinesterase Activity in the Tissues of Zebrafish (Danio rerio)”. If the link does not work for some reason, the PMCID is PMC8066955. Criteria and scoring initial peer review For this peer review, I have put on the left the criteria in a table, and on the right my rating. Some scores are binary, and some scores are a value. The binary scores are yes/no answers, the value question have more text. Criteria Score Study Purpose Yes Data Availability Statement Yes Data location From author upon request Study location Yes, the Department of Food Engineering (Enzymology Laboratory, Biotechnology Laboratory), the Faculty of Agriculture and the Faculty of Fisheries, Cukurova University. Author Review Tier 2/3 Ethics statement Yes Funding statement Yes Code availability No Findings initial peer review In this study we see that the author has been very thorough with his work. He has included most of the criteria that is expected. He has a clear study purpose, written down in a well defined title and abstract. The author review includes institution email as personal email and telephone number. The data availability statement is shown in the top, but also again on the bottom. It is upon request to the author that we could receive it. The ethics statement is written down in the materials and methods, which says: “Experimental procedures were conducted in accordance with the protocols approved by the Ethics Committee of the Çukurova University Faculty of Medicine Experimental Medicine Research and Application Centre (approval code: 3; approval date: 4 July 2018).” The funding statement is also included on the bottom part of the paper, but sadly not the code availability. Methods &amp; Materials initial peer review The methods and materials that are performed and used are carefully explained in chapter 2, with subchapters 2.1 till 2.8 which each explain the importance of the methods and the used materials. The author describes the why, the what, the how much, and the when. This is important in writing a paper because this makes it reproducible. Results initial peer review The results seem to be carefully selected, and annotated nicely in multiple tables and graphs with additional explanations below those tables and graphs. With these visualizations the author concludes 3 important findings. These are the findings of the LC50, the changes of the used enzyme in the brain, and the changes in enzyme activity in the muscles. There is for us sadly no data to hold against it, because this has to be on request by the author. The second peer review with code and data 5.0.1 OSF paper For the second peer review I have used a paper from OSF titled: “Covid-19 Vaccine Paper”, which had to include code and data. The code has been noted as a Rmarkdown file called “covid-vaccine-code_2020-10-04.Rmd”, and the shared data is called “Data_share_covid-19-vacanation_2020-10-04.csv”. Upon looking closely at the code, it is seen that first the data has been imported to be set at correct levels for testing. It seems that the data set and the corresponding code are answers from a survey that have been answered by multiple countries. When looking globally at the code, we can see that it is separated firstly with chunks that are prepared in logical order. You first start of with the setup for the whole Rmd file with the knitr_opts package, and then the data import. After this the analysis takes place in seperate chunks for every question and for every regression analysis and the created table. I personally would give this code a readability score of 5 (very good). When looking at the code it all seemed good, but when trying to run the Rmd file it gave an error for specifying the pathway to the data file. after this problem was fixed the following code in the Rmd was giving me errors. The code looked as followed: #case factor data$case_fct &lt;- cut(as.numeric(data$casesperm), c(-Inf,2000,4000,Inf),labels = c( &quot;low&quot;, &quot;medium&quot;, &quot;high&quot; )) In this case it gave the following error: “Error in $&lt;-.data.frame(*tmp*, case_fct, value = integer(0)) : replacement has 0 rows, data has 13426” I did not really know how to fix this error, so I commented it out. It would mean that there was 1 less plot, but that was not such a big deal because there were plots before that already. Multiple even. There was 1 clumsy mistake in the .Rmd and that was that the libraries where not in there, the Rmd did not already contain the library ggplot2 for instance, which was needed for the graphs. Also the ggsave function did not work, which I also commented out because that was also not necessary. Trying to make it reproducible was not as easy as it looked. As told above it was needed to comment out 2 functions entirely because the problems that arose were not an easy fix. I personally would give this code a reproducibility a 3 (not easy/not hard), because 2 problems where not hard, and 2 problems were (for me) not fixable as quickly as normally possible. The following code was used for the first reproducibility and for creating the first graph: # Added libraries by me (Nicky) because the original Rmd did not include any libraries. library(dplyr) library(tidyverse) library(base) library(ggplot2) # The require options where here, but where not enough to directly reproduce the Rmd. require(tidyverse) require(MASS) # Import the data. data &lt;- read.csv(&quot;/Users/nickyklaver/Downloads/Data_share_covid-19-vaccination_2020-10-04.csv&quot;,fileEncoding = &quot;latin1&quot;) #fix all data$Gender_r &lt;- factor(ifelse( data$Gender == 1, &quot;Male&quot;, ifelse(data$Gender == 2, &quot;Female&quot;, &quot;Other&quot;) ),ordered = FALSE) data$within_fct &lt;- factor(data$within_country,levels = c( 1,2,3,4,5,6), labels = c( &quot;&lt;20%&quot;, &quot;20-40%&quot;, &quot;40-60%&quot;, &quot;60-80%&quot;, &quot;&gt;80%&quot;, &quot;Refused&quot; )) data$ww_fct &lt;- factor(data$world_wide, levels = c( 1,2,3,4,6), c( &quot;&lt;$2 per day&quot;, &quot;$2-$8 per day&quot;, &quot;$8-$32 per day&quot;, &quot;$32+&quot;, &quot;Refused&quot; )) data$educ_fact &lt;- factor(data$Universal_edu, levels = c( 1,2,3,4 ), labels = c( &quot;Less than high school&quot;, &quot;High school some college&quot;, &quot;Bachelor&quot;, &quot;Post Graduate&quot; )) data$country_name &lt;- as.factor(ifelse( #this is by far one of the most egregios pieces of code I have written please forgive me I was tired and hit a wall and now I am too lazy to change it to something more elegant (Written by author) data$Country == 1,&quot;Brazil&quot;, ifelse(data$Country == 2, &quot;Canada&quot;,ifelse( data$Country == 3, &quot;China&quot;,ifelse( data$Country == 4, &quot;Ecuador&quot;,ifelse( data$Country == 5, &quot;France&quot;,ifelse( data$Country == 6, &quot;Germany&quot;,ifelse( data$Country == 7, &quot;India&quot;,ifelse( data$Country == 8, &quot;Italy&quot;,ifelse( data$Country == 9, &quot;Mexico&quot;,ifelse( data$Country == 10, &quot;Nigeria&quot;,ifelse( data$Country == 11, &quot;Poland&quot;,ifelse( data$Country == 12, &quot;Russia&quot;,ifelse( data$Country == 13, &quot;South Africa&quot;, ifelse( data$Country == 14, &quot;South Korea&quot;,ifelse( data$Country == 15, &quot;Singapore&quot;, ifelse( data$Country == 16, &quot;Spain&quot;, ifelse( data$Country == 17, &quot;Sweden&quot;,ifelse( data$Country == 18, &quot;United Kingdom&quot;,ifelse( data$Country == 19, &quot;United States&quot;,&quot;l&quot; )))))))))))))))))))) data$agegroup_fct &lt;- factor(x = data$Age_grou, levels = c(1, 2, 3, 4), labels = c(&quot;18-24&quot;, &quot;25-54&quot;, &quot;55-64&quot;, &quot;65+&quot;), ordered = FALSE) #mortlity factor data$mortality_fct &lt;- cut(as.numeric(data$mortalityperm), c(-Inf,200,400,Inf),labels = c( &quot;low&quot;, &quot;medium&quot;, &quot;high&quot; )) #case factor /// commentend out by Nicky #data$case_fct &lt;- cut(as.numeric(data$casesperm), #c(-Inf,2000,4000,Inf),labels = c( # &quot;low&quot;, # &quot;medium&quot;, # &quot;high&quot; #)) ###just the index vars data_test &lt;- data[,4:13] ###make group data # need to make pop covid and need to make pop COVID data$Busines2_fct &lt;- factor(data$Business2, levels = c( 1, 2, 3, 4, 5 ), labels = c( &quot;Completely disagree&quot;, &quot;Somewhat disagree&quot;, &quot;Neutral/no opinion&quot;, &quot;Somewhat agree&quot;, &quot;Completely agree&quot; )) summary(data$Busines2_fct) ## Completely disagree Somewhat disagree Neutral/no opinion Somewhat agree ## 1179 2299 3488 4579 ## Completely agree ## 1881 #busi by gender gender &lt;- ggplot( data = data )+ geom_bar(aes( x = Gender_r, fill = Busines2_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Business by gender&quot;)+ xlab(&quot;Proportion&quot;)+ ylab(&quot;Gender&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) gender "],["the-easyread-r-package.html", "6 The easyRead R package", " 6 The easyRead R package If I tell you I am a R programmer, you would probably ask me what packages I made. So before you can ask me that question, I made a small R package. I created an R package that can be used to normalize and summarize data + impute missing values and as last detect outliers through the standard IQR method. This package was build by Nicky Klaver (me), and it consists of all of the following: A DESCRIPTION file in proper format, 4 different functions that I wrote, including Roxygen2 documentation in comments, A NAMESPACE file, that was generated by performing the devtools::document() function, One raw dataset that is documented and cleaned. It is accesible by typing data(clean_R_package_data, easyRead_package), A vignette that is accessible by typing browseVignettes(easyRead_package), A installation of the package by typing devtools::install_github(\"NickyKlaver/easyRead_package\"). The created package can be found on the following github page here. "],["project-introduction-pedriactic-cancer.html", "7 Project introduction: Pedriactic Cancer cfDNA R shiny app", " 7 Project introduction: Pedriactic Cancer Over the past decade, we have witnessed a change from traditional invasive techniques for diagnosing and monitoring cancer to more innovative and non-invasive methods, such as liquid biopsies. This new approach has revolutionized the field of clinical oncology, offering significant benefits such as simplified tumor sampling and personalized therapeutic strategies. Liquid biopsies involve the isolation and analysis of tumor-derived samples, such as circulating tumor cells (CTC) and circulating tumor DNA (ctDNA), from the bodily fluids of cancer patients. Once these biomarkers are extracted, they provide invaluable information about tumor characteristics such as progression, staging, heterogeneity, gene mutations, and clonal evolution. Thus, liquid biopsies have improved our understanding of tumors, leading to better detection, ongoing monitoring, and personalized treatment. cfDNA Cell-free DNA (cfDNA) is a term used to describe small fragments of DNA that circulate freely in bodily fluids such as blood, urine, and cerebrospinal fluid. Unlike DNA contained within intact cells, cfDNA is released into the bloodstream through various physiological processes, including apoptosis, necrosis, and active secretion. The origins of cfDNA are diverse and can include contributions from normal cells, as well as cells undergoing pathological processes such as cancer. In the context of liquid biopsy, which involves the analysis of biomarkers in bodily fluids for diagnostic or monitoring purposes, cfDNA holds particular significance. In cancer patients, cfDNA can contain genetic mutations or other alterations that are characteristic of the tumor. These small fragments of DNA are called ctDNA. By isolating and analyzing cfDNA from a blood sample, clinicians can gain valuable insights into the genetic profile of a patient’s tumor, potentially facilitating early detection, monitoring of disease progression, assessment of treatment response, and detection of resistance mutations. R shiny app The R shiny app: A place where you can “make an interactive web application that executes R code on the backend”. The functionality and user friendly interactivity of the R shiny is needed for our project for 2 main reasons. When looking at data, you need to be able to visualize it. Then when this data is visualized, you are still able to give user inputs that can adjust and modify the visualization and results that you are looking for. When working with the ct- and cf-DNA, we want to be able to really visualize and capture what this data means for us. But in the mean time we also would like to give inputs for our charts and graphs, that makes it so that we can really pinpoint on the things we are sought after. By having a web application that can run R calculations on the background, we can create an user friendly environment, that can be used by more than just data scientist. "],["parmaters-showcased-on-ecdc-results.html", "8 Parmaters showcased on ECDC results Introduction", " 8 Parmaters showcased on ECDC results Introduction When performing an analysis on a data set, it is often smart to add parameters to your Rmarkdown file. This is because the use of parameters create a easy and simple way to perform different analysis forms (inputs) on the same data set. In this Rmarkdown I will show you how a parameter can influence the different outcomes of the analysis because of variable change. The used dataset is form European Centre for Disease Prevention and Control (ECDC) and can be found here. We are now generating a report on Covid-19 cases in the year 2022 and the month May for the country Netherlands. # Create the graph for the cases within the parameters. Cases_graph &lt;- ECDC_data %&gt;% group_by(countriesAndTerritories) %&gt;% filter(year == params$year &amp; countriesAndTerritories == params$country &amp; month == params$month) %&gt;% ggplot() + geom_line(aes(y= cases, x = day))+ labs(title = paste(&quot;A graph of all COVID-19 cases in the country&quot;, params$country), subtitle = paste(&quot;in the year&quot;, params$year, &quot;and the month&quot;, month))+ xlab(paste(&quot;The days in month&quot;, params$month))+ ylab(paste(&quot;The amount of COVID-19 cases&quot;)) # Showcase the cases graph within the parameters. Cases_graph We are now creating the same report but on Covid-19 deaths in the year 2022 and the month May for the country Netherlands. # Create the deaths graph within the parameters. Death_graph &lt;- ECDC_data %&gt;% group_by(countriesAndTerritories) %&gt;% filter(year == params$year &amp; countriesAndTerritories == params$country &amp; month == params$month) %&gt;% ggplot() + geom_line(aes(y= deaths, x = day))+ labs(title = paste(&quot;A graph of all COVID-19 deaths in the country&quot;, params$country), subtitle = paste(&quot;in the year&quot;, params$year, &quot;and the month&quot;, month))+ xlab(paste(&quot;The days in month&quot;, params$month))+ ylab(paste(&quot;The amount of COVID-19 deaths&quot;)) # Showcase the deaths graph within the parameters. Death_graph At last we are also intereseted in the amount of cases in the year 2022 and the month May for all the countries in the dataset. # Create boxplot for all the cases in all the countries within the parameters. cases_countries &lt;- ECDC_data %&gt;% group_by(countriesAndTerritories) %&gt;% filter(year == params$year, month == params$month) %&gt;% ggplot() + geom_boxplot(aes(x = cases, y = countriesAndTerritories, fill = countriesAndTerritories))+ labs(title = &quot;A graph of all COVID-19 cases in the submitted countries&quot;, subtitle = paste(&quot;In month&quot;, month,&quot;, in the year&quot;, params$year))+ xlab(paste(&quot;The amount of cases&quot;))+ ylab(paste(&quot;The countries&quot;)) # Showcase all the cases in all the countries within the parameters. cases_countries "],["references.html", "References", " References “Awesome-Single-Cell/README.md at Master \\(\\cdot\\) Seandavi/Awesome-Single-Cell.” n.d. GitHub. https://github.com/seandavi/awesome-single-cell/blob/master/README.md. Accessed May 14, 2024. “Cellgeni/scRNA.seq.course.” 2024. Cellular Genetics Informatics. Chapter 3 Getting scRNA-seq Datasets Introduction to Single-Cell Analysis with Bioconductor. n.d. Accessed May 14, 2024. “Dot Plot Visualization — DotPlot.” n.d. https://satijalab.org/seurat/reference/dotplot. Accessed May 26, 2024. “S3 Browser Singlecellcoursedata/.” n.d. https://singlecellcourse.cog.sanger.ac.uk/index.html?shared=data/. Accessed May 15, 2024. “Scale and Center the Data. — ScaleData.” n.d. https://satijalab.org/seurat/reference/scaledata. Accessed May 26, 2024. SIB - Swiss Institute of Bioinformatics. 2023a. “Single Cell Transcriptomics - Analysis Tools and Quality Control (3 of 10).” ———. 2023b. “Single Cell Transcriptomics - Cell Type Annotation (7 of 10).” ———. 2023c. “Single Cell Transcriptomics - Clustering (6 of 10).” ———. 2023d. “Single Cell Transcriptomics - Differential Gene Expression and Enrichment Analysis (8 of 10).” ———. 2023e. “Single Cell Transcriptomics - Dimensionality Reduction (4 of 10).” ———. 2023f. “Single Cell Transcriptomics - Introduction to Single Cell RNA-seq (1 of 10).” "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
